<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Time Series 调研, WeiLog">
    <meta name="description" content="Follow your heart and intuition, and document your journey through writing and blogging.">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Time Series 调研 | WeiLog</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">WeiLog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">WeiLog</div>
        <div class="logo-desc">
            
            Follow your heart and intuition, and document your journey through writing and blogging.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Time Series 调研</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Time-Series-LLM/">
                                <span class="chip bg-color">Time Series, LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-04
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="时间序列调研"><a href="#时间序列调研" class="headerlink" title="时间序列调研"></a>时间序列调研</h1><h2 id="时序背景介绍"><a href="#时序背景介绍" class="headerlink" title="时序背景介绍"></a>时序背景介绍</h2><h3 id="概念定义"><a href="#概念定义" class="headerlink" title="概念定义"></a>概念定义</h3><p>什么是“时间序列”？</p>
<p>  —— 简单说就是按时间顺序排列的数据，比如每天的气温、每小时的电力消耗、股票的实时价格。</p>
<h3 id="时间序列数据分类"><a href="#时间序列数据分类" class="headerlink" title="时间序列数据分类"></a>时间序列数据分类</h3><ol>
<li>数据特性</li>
</ol>
<p>时序数据本身具有以下特性：顺序性、自相关性、趋势、季节性、周期性、异常值、不规则采样、不平稳性</p>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/x4.png" alt="Refer to caption"></p>
<ol start="2">
<li>多维特性</li>
</ol>
<p>多变量的时间序列数据还具有以下特性：变量间依赖性、受外源潜在变量影响、上下文信息等多源因素的影响。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/x5-20250904153051590.png" alt="Refer to caption"></p>
<h3 id="时间序列任务分类"><a href="#时间序列任务分类" class="headerlink" title="时间序列任务分类"></a>时间序列任务分类</h3><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><p>根据整个序列的模式将其归入某一类别，在故障诊断、行为识别等领域中用到。</p>
<img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250903144342019.png" alt="image-20250903144342019"  />

<p>主要挑战在于：序列通常仅由少量数值点构成单次观测，单个时刻信息有限，需要考虑<strong>时序动态模式</strong>进行判别；不同类别序列的<strong>对齐与变长</strong>问题。</p>
<blockquote>
<p><strong>早期方法</strong>：依赖距离度量➕模版匹配；特征提取➕传统分类器。问题：对复杂模式难以分类，泛化性不强。</p>
<p><strong>机器学习方法</strong>：SVM、频域分析改进分类效果。问题：在捕获长时依赖和非线性动态方面存在局限。</p>
<p><strong>深度学习方法</strong>：CNN局部特征✅但长期依赖❌、RNN（LSTM）存在梯度消失&#x2F;爆炸、Transformer需求数据量多，相对模型计算效率不高。</p>
</blockquote>
<h4 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h4><p>旨在在时间序列中发现不符合正常模式的异常行为或异常点，在工业设备监测、网络安全、医疗监护等领域非常重要。&#x3D;&#x3D;本质上和分类任务类似（二分类）&#x3D;&#x3D;。</p>
<img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/1*Ww4-ErIuYZL9gtwTrTfgXQ-20250904153124612.jpeg" alt="Anomaly detection based on Z-score"  />

<p>主要难点在于：<strong>异常样本稀少且多样</strong>，缺乏充足的有标签异常数据来监督模型训练；异常通常嵌藏在大量正常数据中，<strong>高罕见概率</strong>导致监督学习偏斜；同时<strong>异常定义依赖场景</strong>且会<strong>随时间演化</strong>（概念漂移），给模型带来持续适应挑战。</p>
<blockquote>
<p><strong>传统方法</strong>：基于统计阈值和信号处理（频谱分析）</p>
<p><strong>机器学习方法</strong>：One-Class SVM、无监督聚类方法</p>
<p><strong>深度学习方法</strong>：基于预测的异常检测方法，本质上还是对比误差（阈值）</p>
</blockquote>
<h4 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h4><p>根据过去序列数据预测未来值，在金融、供应链、气象等领域有重要作用。时间序列预测需要应对<strong>趋势和季节性</strong>模式、<strong>多步预测误差累积</strong>、<strong>多变量关联</strong>等核心挑战。</p>
<img src="https://arxiv.org/html/2411.05793v1/x6.png" alt="Refer to caption" style="zoom: 67%;" />

<blockquote>
<p><strong>经典统计方法</strong>：以统计模型为主，ARIMA结合自回归和滑动平均。解释性强且需要数据量少，但仅在简单数据上有效。</p>
<p><strong>机器学习方法</strong>：回归模型中，过去观测为特征，未来值为标签。</p>
<p><strong>深度学习方法</strong>：seq2seq多步预测→序列学习。Transformer改进版本Informer等，在特定数据集上面临过拟合和数据依赖。</p>
</blockquote>
<h4 id="生成建模"><a href="#生成建模" class="headerlink" title="生成建模"></a>生成建模</h4><p>要求学得时间序列数据的分布，从而可以<strong>生成</strong>类似于真实的数据序列。这在模拟场景、数据扩增、隐私数据合成等方面有应用，例如生成真实风格的合成语音、模拟股票走势数据等。&#x3D;&#x3D;本质上和预测任务类似。&#x3D;&#x3D;</p>
<img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250902195858623.png" alt="image-20250902195858623" style="zoom:67%;" />

<p>主要挑战在于：时间序列通常存在<strong>复杂的短期相关和长期结构</strong>，生成模型必须捕捉这种多尺度依赖；生成的不确定性很高，可能存在<strong>多峰分布</strong>（例如未来可能有多种走向），模型需表达多模态输出；此外，评估生成序列质量往往困难，没有明确的客观指标，比图像生成的视觉保真度更难衡量。</p>
<h3 id="时序问题的痛点总结"><a href="#时序问题的痛点总结" class="headerlink" title="时序问题的痛点总结"></a>时序问题的痛点总结</h3><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250902183038769.png" alt="image-20250902183038769" style="zoom:50%;" />

<h4 id="数据层面共性问题"><a href="#数据层面共性问题" class="headerlink" title="数据层面共性问题"></a>数据层面共性问题</h4><ol>
<li><p><strong>采样不规则、缺失与对齐</strong>：在现实中，由于采样过程不准确，数据往往存在间隔不等、变量异步、单位测量变化、缺失值等诸多问题，这将严重影响数据建模的分布情况，若预处理不当，脏数据将严重影响模型性能。</p>
<p> 举例：在医疗健康领域，由于存在多源异步采样（生命体征、实验室检查、用药）、<strong>大量缺失且“信息性缺失”</strong>（测不到&#x2F;不测往往本身有含义）、噪声与测量误差并存，随病程出现<strong>明显非平稳</strong>。</p>
</li>
<li><p><strong>分布漂移（Distribution Shift）</strong>：<strong>时间序列数据的非平稳性</strong>导致分布随时间发生变化，这是最具挑战性的痛点之一。现实世界并非静态的，事物在变化，数据分布会漂移。包括<strong>协变量漂移</strong>：输入特征分布发生变化；<strong>先验概率漂移</strong>：目标变量分布变化；<strong>概念漂移</strong>：输入与输出间关系发生变化。</p>
<p> 举例：在工业传感器维护过程中，传感器数量多、<strong>工况切换频繁</strong>（负载、转速、环境），<strong>跨机型&#x2F;批次的域间差异</strong>显著；寿命数据呈”运行到失效”的长尾形态。</p>
</li>
<li><p><strong>尺度依赖与长序列建模</strong>：现实中的时间序列往往呈现出<strong>长时间跨度</strong>，其中既包含快速变化的局部模式（如突发事件、短周期波动），也蕴含缓慢演化的全局趋势（如病程进展、设备老化、市场长期走势）。</p>
<p> 举例：在金融交易场景中，股票价格序列可能跨越数年，包含日内高频波动（毫秒&#x2F;分钟级）与宏观经济周期（季度&#x2F;年度级）；在交通预测中，需要同时建模局部的高峰拥堵模式与长期的出行结构变化。</p>
</li>
<li><p><strong>多变量关联与外生驱动</strong>：多变量之间的时滞相关、共因与混杂使“相关≠因果”。<strong>变量间复杂依赖和外部因素影响</strong>构成了多变量时序建模的核心难题。</p>
<p> 举例：电力价格高度依赖于市场供需关系，仅基于历史数据预测未来价格在本质上是不可能的，需要纳入经济指标、政策变化等外生变量（金融市场交易更是如此）。</p>
</li>
<li><p><strong>标注稀缺、异常稀少、异常定义主观</strong>：实际应用中，异常点极少且定义依赖场景，标注代价高，很多公开基准集的标签与分布也难覆盖真实复杂性，直接迁移会导致评测与落地的“指标—价值错位”。</p>
</li>
</ol>
<h4 id="任务相关的特定痛点"><a href="#任务相关的特定痛点" class="headerlink" title="任务相关的特定痛点"></a>任务相关的特定痛点</h4><table>
<thead>
<tr>
<th>任务类型</th>
<th>核心挑战</th>
<th>常用解决方案</th>
</tr>
</thead>
<tbody><tr>
<td><strong>分类</strong></td>
<td>① <strong>动态模式</strong>主导、单点判别力弱<br>② <strong>变长与相位错位</strong>导致比对困难<br>③ <strong>小样本&#x2F;跨域泛化</strong>差</td>
<td><strong>传统方法</strong>：1NN+DTW动态对齐、手工特征+SVM<br><strong>深度学习</strong>：CNN提取局部模式、原型学习增强泛化<br><strong>预处理技巧</strong>：序列对齐、patch分块化</td>
</tr>
<tr>
<td><strong>预测</strong></td>
<td>① <strong>趋势&#x2F;季节</strong>与短期扰动并存<br>② <strong>多步误差累积</strong><br>③ <strong>长序列</strong>计算负担<br>④ <strong>多变量&#x2F;外生驱动</strong>与<strong>分布漂移</strong></td>
<td><strong>经典统计</strong>：ARIMA&#x2F;ETS分解+线性回归<br><strong>深度学习</strong>：LSTM记忆门控、Transformer自注意力<br><strong>新兴方法</strong>：线性模型（LTSF-Linear）、扩散模型<br><strong>应对策略</strong>：模型集成、滚动再训练、patch化处理</td>
</tr>
<tr>
<td><strong>异常检测</strong></td>
<td>① <strong>极端不平衡</strong>与<strong>多样形态</strong>，标签稀缺<br>② <strong>概念&#x2F;工况演化</strong>致阈值失稳<br>③ 评测需兼顾<strong>早检出与误报成本</strong></td>
<td><strong>无监督方法</strong>：Isolation Forest、One-Class SVM<br><strong>深度学习</strong>：预测残差分析、自编码器重构误差<br><strong>自适应技术</strong>：动态阈值调整、概念漂移监测</td>
</tr>
<tr>
<td><strong>生成建模</strong></td>
<td>① <strong>多峰&#x2F;多模态</strong>未来、不确定性高<br>② <strong>长程一致性</strong>与物理&#x2F;业务约束<br>③ <strong>质量评估</strong>缺乏统一指标</td>
<td><strong>传统生成</strong>：HMM隐马尔可夫、状态空间模型<br><strong>深度生成</strong>：TimeGAN对抗训练、VAE变分自编码器<br><strong>扩散模型</strong>：TimeGrad概率预测、CSDI条件生成</td>
</tr>
</tbody></table>
<h2 id="时序方法发展脉络"><a href="#时序方法发展脉络" class="headerlink" title="时序方法发展脉络"></a>时序方法发展脉络</h2><p><strong>时序方案发展趋势</strong>：</p>
<ul>
<li><strong>从手工到自动</strong>：特征工程→端到端学习</li>
<li><strong>从单一到混合</strong>：单模型→集成&#x2F;多尺度&#x2F;分解式组合</li>
<li><strong>从确定到概率</strong>：点预测→分布&#x2F;不确定性建模</li>
<li><strong>从固定到自适应</strong>：静态模型→在线学习&#x2F;概念漂移适应</li>
</ul>
<h3 id="大模型前的发展"><a href="#大模型前的发展" class="headerlink" title="大模型前的发展"></a>大模型前的发展</h3><p>时间线：从20世纪后期的统计模型（ARIMA、HMM等），到21世纪初的机器学习方法（SVM、随机森林等），再到近十年的深度学习模型（RNN&#x2F;LSTM、CNN、Transformer等），方法不断演进以应对数据规模和复杂性的增长。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/tsl%E6%96%B9%E6%B3%95%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C.png" alt="tsl方法发展脉络"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250902112220315.png" alt="image-20250902112220315"></p>
<h4 id="传统统计-机器学习"><a href="#传统统计-机器学习" class="headerlink" title="传统统计 &amp; 机器学习"></a>传统统计 &amp; 机器学习</h4><p>最早的时序预测以<strong>指数平滑</strong>、<strong>ARIMA&#x2F;SARIMA</strong>为代表，依赖平稳性与线性自回归&#x2F;滑动平均假设；随后出现<strong>树模型、SVM</strong>等机器学习范式以刻画部分非线性关系，但整体仍难以充分捕捉复杂长期依赖与多变量交互。</p>
<h4 id="传统深度学习"><a href="#传统深度学习" class="headerlink" title="传统深度学习"></a>传统深度学习</h4><ul>
<li><strong>RNN&#x2F;LSTM</strong>强化记忆门控以减轻梯度问题、增强长依赖；</li>
<li><strong>TCN&#x2F;CNN</strong>用空洞卷积与大感受野高效建模长序列；</li>
<li><strong>GNN</strong>在图结构下学习跨通道关系。</li>
</ul>
<p>这些架构能学到更复杂的时序模式，但也受各自”归纳偏置”所限（如RNN顺序递归瓶颈、CNN局部性）。</p>
<h4 id="Transformer-Advanced-Transformer"><a href="#Transformer-Advanced-Transformer" class="headerlink" title="Transformer &amp; Advanced Transformer"></a>Transformer &amp; Advanced Transformer</h4><p>自注意力善于处理&#x3D;&#x3D;序列长程依赖&#x3D;&#x3D;（时序数据中的长序列和LLM中的长上下文的“长”不在同一个量级），迅速扩展到时序预测并取得强势表现；</p>
<p>围绕自注意力的可扩展性与时序归纳偏置进行增强：</p>
<ul>
<li><p><strong>Patching</strong> 提升长序列效率；提升长距离预测能力。</p>
</li>
<li><p><strong>Cross-dimension</strong> 显式建模多通道相关性；提升多尺度能力。</p>
</li>
<li><p><strong>Exogenous</strong>（外生条件&#x2F;先验）与<strong>Additional Approaches</strong>（如稀疏注意力等）进一步贴合多变量、异质分布的数据特性。</p>
</li>
</ul>
<div style="display:flex; gap:4px;">
  <img src="/Users/weichy/Library/Application Support/typora-user-images/image-20250902130749743.png" style="width:50%;">
  <img src="/Users/weichy/Library/Application Support/typora-user-images/image-20250902130709254.png" style="width:50%;">
</div>


<p>在“注意力可扩展性 + 时序归纳偏置”上做强化：通过<strong>稀疏化&#x2F;频域变换&#x2F;多尺度结构&#x2F;内置分解</strong>来降低复杂度、显式注入周期与趋势先验，并配合<strong>直接多步</strong>解码以提升长序列预测稳定性。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250903165021370.png" alt="image-20250903165021370"></p>
<blockquote>
<p><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.13008">Autoformer</a></strong>：把<strong>趋势–季节分解</strong>内置到网络中，跨层迭代“去趋势&#x2F;还原趋势”；以<strong>Auto-Correlation</strong>替代点对点注意力，直接在<strong>周期&#x2F;子序列</strong>层面建模长期依赖。Encoder: 每次分解都会把趋势-循环成分剥离，保留更“纯”的季节表示; Decoder: 并行地把每次分解得到的趋势分量<strong>经投影器加入“趋势累积”</strong></p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250903165656022.png" alt="image-20250903165656022"></p>
<blockquote>
<p><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.12740">FEDformer</a></strong>：在<strong>频域&#x2F;小波域</strong>进行子带选择与交互（Frequency-Enhanced Block&#x2F;Attention），再逆变换回时域；与<strong>序列分解</strong>结合，突出低频趋势与主要周期，近线性代价生成长跨度预测。</p>
</blockquote>
<p>&#x3D;&#x3D;和大模型不同的是，这些Transformer还是端到端、确定数据集训练的小模型。&#x3D;&#x3D;</p>
<p><strong>问题</strong>：研究发现<strong>简单线性&#x2F;分解式模型在若干基准上可与甚至超越Transformer</strong>，引发对”何为时序有效归纳偏置”的再思考。</p>
<blockquote>
<p>“归纳偏置” <strong>inductive bias</strong>。它指的是学习算法&#x2F;模型<strong>先验地带入的一套假设</strong>与约束——决定了模型更容易学习哪类模式、忽略哪类模式，从而影响在有限数据上的泛化。</p>
<blockquote>
<p>在时序建模里，可以把“归纳偏置”理解为：模型对<strong>时间顺序、平稳性、局部性&#x2F;周期性、跨变量关系</strong>等特性的内置假设与结构设计，如：<strong>RNN&#x2F;LSTM</strong>：结构上按时间<strong>顺序递推</strong>，隐状态携带历史，这天然把“时间有序连续”作为偏置；门控机制则偏向保留长期关键信息。<strong>CNN&#x2F;TCN</strong>：卷积的<strong>局部性与多尺度</strong>使其偏向捕捉邻域模式、周期&#x2F;季节性等局部规律，常与强调全局关系的模型形成互补。<strong>简单线性&#x2F;分解式模型（LTSF-Linear 等）</strong>：偏向<strong>趋势+季节性</strong>成分的直接提取与<strong>严格的时间顺序保真</strong>，在一些长序列任务上胜过复杂注意力模型。</p>
</blockquote>
<p>“线性可赢“ <strong>linear wins</strong>。在<strong>长时序预测（LTSF）等任务上，极其简单的线性模型（例如对分解后的序列做线性映射）在若干基准上打败了</strong>不少基于 Transformer 的复杂模型，引发了社区对“什么才是时序合适的归纳偏置”的反思。</p>
</blockquote>
<h4 id="Advanced-Traditional-DL"><a href="#Advanced-Traditional-DL" class="headerlink" title="Advanced Traditional DL"></a>Advanced Traditional DL</h4><p>在”线性可赢”的启发下，对<strong>MLP&#x2F;CNN&#x2F;RNN&#x2F;GNN</strong>进行**轻量化、混合式与模型无关（model-agnostic）**改造，重点在于&#x3D;&#x3D;利用时序本身特征，提高建模效率&#x3D;&#x3D;：</p>
<ul>
<li><p><strong>混合方法（Hybrid）</strong>：将线性趋势&#x2F;季节分解与深度特征提取结合；典型思路是先做<strong>时间序列分解</strong>（趋势、季节&#x2F;周期、残差），然后各自建模再重组；也有把<strong>小波&#x2F;频域</strong>分解出来的分量交给CNN&#x2F;GNN&#x2F;MLP&#x2F;Transformer分头处理再融合的做法。</p>
  <img src="https://github.com/cure-lab/LTSF-Linear/raw/main/pics/Linear.png" alt="image" style="zoom:33%;" />

<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.13504"><strong>LTSF-Linear&#x2F;DLinear&#x2F;NLinear</strong></a>：通过最简单的线性加权、加上趋势&#x2F;季节分解或归一化，就能在不少基准任务上接近甚至超越复杂的 Transformer。</p>
<p>DLinear: 趋势–季节分解 + 两个 Linear。</p>
<p>NLinear：简单归一化 + Linear + 还原操作。</p>
<p>我之前参与的关于ESN的研究本质上就是利用周期性结合RNN来优化时序特征提取过程。</p>
</blockquote>
</li>
<li><p><strong>更高效的CNN&#x2F;TCN或门控RNN</strong>：用于长期依赖与局部模式的折中；以卷积提取<strong>局部&#x2F;多尺度</strong>模式，时间与显存开销都低于自注意力，且与时序的<strong>周期&#x2F;季节性</strong>很匹配；常与全局方法互补。</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.04939">Inception Time</a> - CNN变体：多尺度+残差增强。</p>
<p>Inception block详细如下：<strong>Bottleneck</strong>：对每个时间步做 <strong>1×1 卷积</strong>（即仅沿<strong>通道维</strong>线性变换，不在时间上卷积），把通道从 $M$ 压缩到 $m$，图中m取1。随后接<strong>不同长度 $l\in{10,20,40}$ 的 1D 卷积核</strong>（都沿时间维），再加一个 <strong>MaxPooling 分支</strong>。四个分支的输出在通道维<strong>拼接</strong>，形成该 Inception 模块的输出。</p>
</blockquote>
<p>  <img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250902172308102.png" alt="image-20250902172308102"></p>
<blockquote>
<p>经典的<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.02186">TimesNet</a>将时序数据2维化后进行卷积</p>
</blockquote>
  <img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250902165508312.png" alt="image-20250902165508312" style="zoom:50%;" />
</li>
<li><p><strong>图神经网络（GNN）</strong>：显式表达通道图结构，能把多变量时序的<strong>通道相关性</strong>建成图，<strong>同时</strong>学习时空关系。</p>
</li>
</ul>
<blockquote>
<p>“模型无关”不是指”不要模型”，而是指<strong>改进策略与技巧</strong>不绑定某一类主干。”特征提取&#x2F;分解&#x2F;频域变换”等做法，不局限于特定架构，可跨多种模型复用。</p>
</blockquote>
<h4 id="Diffusion-Models"><a href="#Diffusion-Models" class="headerlink" title="Diffusion Models"></a>Diffusion Models</h4><p>扩散模型通过“正向加噪→反向去噪”的两步生成，<strong>前向过程</strong>把真实未来片段逐步加上高斯噪声；<strong>反向过程</strong>从随机噪声出发，借助一个<strong>去噪网络</strong>一步步去噪得到未来序列。</p>
<blockquote>
<p><strong>DDPM 的核心是学一个“估计噪声$\hat\epsilon_\theta(x_t,t,c)$ ”的单步去噪器</strong> ，然后在采样时把上一步的噪声减掉、按噪声日程一步步“从噪声走回数据”。</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250902134900608.png" alt="image-20250902134900608" style="zoom:50%;" />

<p>做预测时采用<strong>条件生成</strong>：用历史序列作为条件，让反向去噪朝着与历史一致的未来演化。</p>
<blockquote>
<p><strong>条件只喂给去噪网络，而前向噪声本身仍是固定的高斯加噪过程。时序预测通常只对”未来窗口”加噪、历史只作为条件输入。</strong></p>
</blockquote>
<p>Diffusion在时序（主要是预测）中从”<strong>条件怎么喂</strong>“（编码、未来先验）与”<strong>特征怎么抽</strong>“（分解&#x2F;频域&#x2F;多尺度）两端同时演进，目标是把<strong>历史先验+结构特征+去噪网络</strong>三者对齐，从而既保留趋势&#x2F;周期，又能刻画不确定性。</p>
<p>&#x3D;&#x3D;采用Diffusion视角的TS是一个概率分布问题，而DL&#x2F;Transformer系方法是回归式建模。Diffusion有更强的生成能力，适合 <strong>多模态、不确定性显著的预测</strong>。&#x3D;&#x3D;</p>
<h4 id="Mamba-Models"><a href="#Mamba-Models" class="headerlink" title="Mamba Models"></a>Mamba Models</h4><ul>
<li><p><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.00396">SSM</a> 离散状态空间模型</strong>：把<strong>序列建模问题转化为一个线性系统的卷积问题</strong>。相比于Transformer、RNN系列模型，SSM具有”长依赖+高效率”的特性。</p>
  <img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250902153537920.png" alt="image-20250902153537920" style="zoom: 50%;" />

<p>  SSM把序列建模<strong>还原为一个线性时不变（LTI）系统的卷积核问题</strong>，离散定义为：<br>  $$<br>  x_t&#x3D;Ax_{t-1}+Bu_t,\qquad y_t&#x3D;Cx_t+Du_t<br>  $$<br>  其中 $A$ 决定了记忆的“保留时间”。逐步展开后，得到：<br>  $$<br>  y_t &#x3D; \sum_{k \ge 0} K_k, u_{t-k} + D u_t, \quad K_k &#x3D; C A^k B<br>  $$<br>  也就是说，输出 $y_t$ 并不是只依赖于最近的输入，而是依赖于<strong>所有过去的输入</strong>，只不过”远处的记忆”会随着 $A^k$ 衰减（不同于马尔可夫性，如RNN模型）。</p>
<blockquote>
<p>长记忆：矩阵 $A$ 的特征值控制衰减速度。</p>
<ul>
<li>如果 $|\lambda| &lt; 1$，那么 $A^k$ 对应的项会以 $\lambda^k$ 形式衰减。</li>
<li>如果 $|\lambda|$ 很接近 1（实部接近 0），衰减就很慢，意味着能记住很久。</li>
<li>如果有一组不同大小的特征值分布，就相当于组合了<strong>短期记忆</strong>（快速衰减）和<strong>长期记忆</strong>（慢速衰减），形成多时间尺度记忆机制。</li>
</ul>
</blockquote>
<blockquote>
<p>高效率：</p>
<p>输出公式：$y_t &#x3D; \sum_{k \ge 0} K_k, u_{t-k}$ 其实就是卷积 $y &#x3D; K * u$ </p>
<ul>
<li>在 RNN 里，须一步一步传递信息：从 $t-1$ 推到 $t$，时间复杂度是 $O(T)$</li>
<li>在 Transformer 里，虽然是全局注意力，但计算复杂度是 $O(T^2)$，因为要比较所有 pairwise 的依赖。</li>
<li>在 SSM 里，卷积可以用 <strong>FFT 或者扫描算法</strong>来实现，复杂度是接近 <strong>$O(T \log T)$ 或甚至 $O(T)$</strong>，而且可以<strong>一次性并行计算全序列</strong>，而不需要逐步传递。</li>
</ul>
</blockquote>
<blockquote>
<p>LTI假设：系统是线性且参数不随时间变。</p>
<p>SSM的优势来自：</p>
<ol>
<li><strong>数学最优性</strong>（HiPPO）：正交多项式基提供最优历史压缩</li>
<li><strong>谱优化</strong>：特征值分布实现多尺度记忆</li>
<li><strong>计算效率</strong>（S4）：NPLR+Cauchy实现近线性复杂度</li>
<li><strong>并行性</strong>：LTI特性允许全局卷积</li>
</ol>
</blockquote>
</li>
<li><p><strong><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.00752">Mamba</a></strong>：提出”<strong>选择性SSM</strong>“，由于纯LTI-SSM对输入自适应性不够、在信息稠密的离散领域（如文本）偏弱。让SSM参数<strong>随输入动态变化</strong>以选择性记忆&#x2F;遗忘，从而使得Mamba具有更灵活、无注意力、高吞吐的序列主干；其变体围绕稳定性、通道依赖、与Transformer技巧融合持续演化，并在多篇工作中报告优于Transformer基线。</p>
  <img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250902153304339.png" alt="image-20250902153304339" style="zoom:50%;" />

<blockquote>
<p>让 $\Delta_t,B_t,C_t$ 依输入而变，得到<strong>内容依赖的动态滤波器</strong>：遇到”无关token”就把 $\Delta_t$ 变大以”重置&#x2F;忽略”，遇到关键信息则让状态持续。</p>
</blockquote>
</li>
</ul>
<p>&#x3D;&#x3D;从传统方法到现代方法的发展脉络清晰：<strong>更强的非线性学习能力</strong>、<strong>更长的依赖记忆能力</strong>、<strong>更少的人工干预特征</strong>，以及<strong>更好的不确定性刻画</strong>。&#x3D;&#x3D;</p>
<h3 id="大模型-时序分析"><a href="#大模型-时序分析" class="headerlink" title="大模型-时序分析"></a>大模型-时序分析</h3><h4 id="LLM-TS-背景"><a href="#LLM-TS-背景" class="headerlink" title="LLM-TS 背景"></a>LLM-TS 背景</h4><p>“Can large pretrained models trained on massive amounts of time-series data learn temporal patterns that can be useful for time-series forecasting on previously unseen datasets?”</p>
<h5 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h5><ul>
<li><strong>更强的泛化性（zero-shot）</strong>：预训练时间序列“基础模型”在跨领域、跨频率、跨变量的零样本预测上显示出可迁移性。</li>
<li><strong>更高的数据效率（few-shot）</strong>：参数高效微调将通用LLM对齐到时间序列后，少量标注即可优于专用模型性能。</li>
<li><strong>利用LLM的链式推理与prompt</strong>：通过重编程与”Prompt-as-Prefix”等机制，用自然语言上下文强化时间序列的推理与解释，从语言模型的理解角度检验其对”纯数值预测”是否真正有效。</li>
<li><strong>多模态融合</strong>：将数值序列与文本元数据&#x2F;事件描述等共同建模，有助于场景知识注入与可解释性。</li>
</ul>
<h5 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h5><ul>
<li><p><strong>时序—语言的模态对齐难题</strong>：连续、噪声敏感、非平稳的时间序列与离散、语义稠密的自然语言存在表征偏差；现有对齐路径包括（i）重编程&#x2F;前缀提示（Time-LLM），（ii）量化&#x2F;标记化为“时间序列词表”（Chronos），（iii）指令&#x2F;两阶段对齐（LLM4TS，TimeCMA），但一致而稳健的跨任务对齐仍未解决。</p>
</li>
<li><p><strong>“离散词元 vs. 连续数据”的信息权衡</strong>：量化&#x2F;标记化便于复用LLM架构与采样式概率预测，但可能带来数据精度与分布细节的损失。</p>
</li>
<li><p><strong>知识与推理范式差异</strong>：LLM的语言归纳偏置未必天然迁移到数值动态建模；”序列建模”能力对纯时序任务的提升有限，优势更可能体现在多模态&#x2F;工具增强场景。<strong>本质还是对齐问题没有解决</strong>。</p>
</li>
<li><p><strong>数据与基准的短板</strong>：对于时序数据而言，高覆盖、多域、多频率的预训练语料与统一评测基准仍稀缺。</p>
</li>
</ul>
<h4 id="两种建模范式"><a href="#两种建模范式" class="headerlink" title="两种建模范式"></a>两种建模范式</h4><h5 id="LLM时序离散化-重编程：把时序数据与语言模态对齐"><a href="#LLM时序离散化-重编程：把时序数据与语言模态对齐" class="headerlink" title="LLM时序离散化&#x2F;重编程：把时序数据与语言模态对齐"></a>LLM时序离散化&#x2F;重编程：把时序数据与语言模态对齐</h5><h6 id="TIME-LLM-ICLR-2024"><a href="#TIME-LLM-ICLR-2024" class="headerlink" title="TIME-LLM (ICLR 2024)"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.01728">TIME-LLM (ICLR 2024)</a></h6><p>利用已有的大语言模型骨干网络，通过重编程层实现时序-文本模态对齐。</p>
<p>Time-LLM保持大语言模型主干<strong>冻结</strong>，通过”reprogramming”把连续时序重编码为<strong>文本原型（text prototypes）</strong>，再配合**Prompt-as-Prefix (PaP)**引导LLM对时序patch的表征与转换，最后经投影层回到数值空间做预测。</p>
<img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250903173434209.png" alt="image-20250903173434209" style="zoom:67%;" />

<p><strong>Patch Reprogramming</strong>：把每个patch的向量表示投影到LLM的词向量空间里，用**一小组从词表嵌入中筛选出来的”文本原型”**作为”基底”去重构（加权组合）这些patch，使两种模态在表示空间上对齐。</p>
<p>这些text prototypes可学习到诸如”short up &#x2F; steady down”等语言线索，并以组合方式表示<strong>局部时序模式</strong>（如”先短上升再平稳下降”），相当于用少量语义化”基向量”去编码patch语义。</p>
<div style="display:flex; gap:8px;">
  <img src="/Users/weichy/Library/Application Support/typora-user-images/image-20250903122744788.png" style="width:50%;">
  <img src="/Users/weichy/Library/Application Support/typora-user-images/image-20250903133213482.png" style="width:50%;">
</div>


<p><strong>Prompt-as-Prefix</strong>：把<strong>文本提示</strong>作为<strong>前缀</strong>与经重编程后的patch表达一起输入冻结的LLM；前向后<strong>丢弃前缀部分</strong>，仅用输出表征做线性投影得到预测值，如上图所示。</p>
<p>提示由三部分组成：dataset context、task instruction、input statistics**（趋势、滞后等）**，示例如下图所示：</p>
<img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250903134427208.png" alt="image-20250903134427208" style="zoom:50%;" />

<p><strong>实验结果</strong>：用TIME-LLM预测电力、天气、交通等数据，TIME-LLM在长&#x2F;少样本与跨域预测上整体领先，零样本跨域（用A领域数据训练，直接预测没见过的B领域数据）相较次优基线比传统专用预测模型效果更好。</p>
<h6 id="TimeCMA-AAAI-2025"><a href="#TimeCMA-AAAI-2025" class="headerlink" title="TimeCMA (AAAI 2025)"></a><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/34067">TimeCMA (AAAI 2025)</a></h6><p>TimeCMA面向多变量时间序列预测（MTSF），采用”两支路编码+模态对齐+预测解码”的架构：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250903223940028.png" alt="image-20250903223940028"></p>
<p><strong>双模态编码</strong>：</p>
<ul>
<li><p><strong>时间序列编码（TSE）</strong>：用一个轻量的Transformer Encoder编码历史数值，得到每个变量&#x2F;通道各自的向量表示（它的好处是”按变量分得开”）。</p>
</li>
<li><p><strong>LLM Prompt编码</strong>：把同一段数值写成Prompt，送进<strong>冻结的GPT-2</strong>，只取<strong>最后一个token的向量</strong>作为该变量的提示表示，并且把这个最后token离线缓存，训练&#x2F;推理时复用。</p>
</li>
</ul>
<p><strong>模态对齐（CMA）</strong>：</p>
<p>此处并没有直接将双模态编码后的结果拼起来，而是基于Channel设计了<strong>相似度检索</strong>，即TSE当”检索器”Q，LLM Prompt编码当”资料库”K V，按相似度对V做聚合，再叠加到原始数值特征上，得到对齐后的数据token向量（类似一种交叉注意力的设计）。</p>
<blockquote>
<p><strong>通道级相似度检索（channel-wise similarity retrieval）</strong><br>令</p>
<ul>
<li>$H_T\in\mathbb{R}^{C\times N}$：TSE 输出（按变量组织）；</li>
<li>$L_N\in\mathbb{R}^{N\times E}$：每变量的 LLM <strong>最后 token</strong> 表征。</li>
</ul>
<p>首先用三组线性映射把两模态投影到对齐空间：<br>$$<br>Q&#x3D;\psi_q(H_T),\quad K&#x3D;\psi_k(L_N),\quad V&#x3D;\psi_v(L_N).<br>$$<br>计算<strong>通道-嵌入</strong>二维的相似度矩阵并归一化：<br>$$<br>M_T&#x3D;\mathrm{softmax}\big(Q;\otimes;K\big)\in\mathbb{R}^{C\times E}.<br>$$<br>用该相似度对 $V$ 做聚合并与原时序加入性融合，得到对齐后的表征：<br>$$<br>H_C&#x3D;\omega_c\big(V;\otimes;M_T\big);\oplus;H_T.<br>$$</p>
</blockquote>
<p><strong>预测头</strong>：</p>
<p>将对齐后的 $H_C$ 输入<strong>多变量Transformer解码器</strong>（含自注意与跨注意）以建模变量间长期依赖，最后经线性投影得到未来 $M$ 步的多变量预测并做反归一化。</p>
<p>实验表明，相比Time-LLM的直接拼接，TimeCMA的对齐方式效果更好。</p>
<h5 id="原生时序基础模型：直接读懂时间序列"><a href="#原生时序基础模型：直接读懂时间序列" class="headerlink" title="原生时序基础模型：直接读懂时间序列"></a>原生时序基础模型：直接读懂时间序列</h5><h6 id="Chronos（TMLR-2024）"><a href="#Chronos（TMLR-2024）" class="headerlink" title="Chronos（TMLR 2024）"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.07815">Chronos（TMLR 2024）</a></h6><p>Chronos把实值序列通过<strong>mean scaling+等距量化</strong>变成<strong>有限词表上的token序列</strong>，用<strong>标准T5或GPT2</strong>做next token prediction；推断时<strong>自回归地产生未来tokens</strong>并<strong>映回实数</strong>，多次采样即可得到<strong>概率型预测</strong>。</p>
<img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250903180255402.png" alt="image-20250903180255402" style="zoom:50%;" />

<blockquote>
<p>数值序列经缩放（scaling）与量化（quantization）后，被离散到 $B$ 个bins，每个bin对应<strong>一个离散token id</strong>，即自然数集合 ${1,\dots,B}$。此外还会加入<strong>PAD</strong>与<strong>EOS</strong>两个特殊符号，构成时间序列词表 $V_{\text{ts}}$。</p>
<p>论文将<strong>量化范围</strong>设为 $[-15,+15]$，在区间 $r \in [-15, 15]$ 内创建4096个等间距的Bin，$q(x) &#x3D; \arg\min_j |c_j - x|$，其中$c_j$为第j个Bin的中心。</p>
<p>模型在每个步长预测<strong>下一个token的分布</strong> $p_\theta(z_{C+h+1}\mid z_{1:C+h})$，对真实量化标签做<strong>多类交叉熵</strong>训练：<br>$$<br>\ell(\theta)&#x3D;-\sum_{h&#x3D;1}^{H+1}\sum_{i\in V_{\text{ts}}}\mathbf{1}(z_{C+h+1}&#x3D;i)\log p_\theta(i\mid z_{1:C+h}) .<br>$$<br>在推断过程中，将每个预测token $z$ 映射到中心 $d(z)&#x3D;c_z$，再乘以尺度 $s$（mean scaling的逆变换）得到实值预测。</p>
</blockquote>
<p>Chronos不具备通用的自然语言理解&#x2F;生成能力。</p>
<h6 id="TimesFM（ACM-2024）"><a href="#TimesFM（ACM-2024）" class="headerlink" title="TimesFM（ACM 2024）"></a><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2310.10688">TimesFM（ACM 2024）</a></h6><p><strong>目标</strong>：旨在学习一个统一的零样本预测器 $f:(y_{1:L})\to \hat y_{L+1:L+H}$，在未知的目标数据集上直接以不同历史长度 $L$、不同预测步长 $H$、以及不同时间粒度（分钟&#x2F;小时&#x2F;日&#x2F;周&#x2F;月等）工作。</p>
<p><strong>架构</strong>：</p>
<ul>
<li><p>输入端做<strong>patch化</strong>，每个补丁经一个带跳连的单隐层MLP变换到model_dim维度后，加上位置编码作为Transformer的输入token。</p>
</li>
<li><p><strong>decoder-only Transformer</strong>：模型由标准Transformer构成（多头自注意力+前馈网络），使用causal mask：第 $j$ 个输出只能注意到自身及其之前的token。</p>
</li>
<li><p><strong>输出patch长度 $h$</strong> 可以<strong>大于</strong>输入补丁长度 $p$<strong>（$h&gt;p$）</strong>。每个输出token通过一个输出残差块直接预测<strong>后续 $h$ 个时间步</strong>。在output_patch上取input_patch作为下一轮自回归的输入，便于交叉核对。</p>
</li>
</ul>
<blockquote>
<p>在约100B时间点<strong>规模（实数据+合成）上预训练，模型规模约</strong>200M参数。在多个新数据集的<strong>零样本</strong>预测中，精度接近针对单数据集全监督训练的专用模型。</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250903212827208.png" alt="image-20250903212827208" style="zoom:67%;" />

<h6 id="Time-MoE-ICLR-2025"><a href="#Time-MoE-ICLR-2025" class="headerlink" title="Time-MoE (ICLR 2025)"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.16040">Time-MoE (ICLR 2025)</a></h6><p>TimeMoE以<strong>decoder-only Transformer+稀疏MoE</strong>为骨干，采用<strong>逐点嵌入、RMSNorm、RoPE、Top-k路由+共享专家</strong>实现高容量与低算力并存；在目标端采用<strong>多分辨率自回归</strong>与<strong>贪心调度</strong>支持任意预测长度；基于<strong>Time-300B</strong>的大规模预训练，配合<strong>Huber损失+负载均衡辅助损失</strong>与统一的数据工程&#x2F;采样策略，实现了在<strong>相同激活算力</strong>下优于同规模稠密模型的时序基础模型。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250903232307189.png" alt="image-20250903232307189"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250904105438581.png" alt="image-20250904105438581"></p>
<p>实验效果优于TimesFM、Chronos、Moiral等一众原生大模型。</p>
<h4 id="Agent视角"><a href="#Agent视角" class="headerlink" title="Agent视角"></a>Agent视角</h4><p>用LLM+Agent策略理解时序，进行时序分析。本质上，预测、分类等时序分析问题都是为了在场景下根据分析到的结果做出决策。AI Agent若能将时序分析工具嵌入其中，将赋能金融交易、工业检测等重要活动。</p>
<blockquote>
<p>“a planner agent equipped with structured knowledge banks, curated libraries of models and refinement strategies, which guide exploration, while improving interpretability and reducing error propagation.”</p>
<p>Agent通过结构化知识库、精选模型库和细化策略，可指导相关领域探索，同时提高可解释性并减少错误传播。</p>
</blockquote>
<h6 id="TS-Agent"><a href="#TS-Agent" class="headerlink" title="TS-Agent"></a>TS-Agent</h6><blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13915">Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback</a> 2025.8</p>
</blockquote>
<p>一个面向金融时间序列的模块化智能体框架，通过结构化的推理与反思式反馈自动化完成建模流程。其总体思想是把工作流形式化为包含<strong>模型选择→代码细化→超参微调</strong>的迭代决策过程，并以结构化知识库（案例库、代码库、细化&#x2F;诊断知识库）为外部”支撑”来约束与引导搜索，从而兼顾<strong>性能、可审计性与鲁棒性</strong>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250903111206313.png" alt="image-20250903111206313"></p>
<p><strong>总体两阶段</strong>：</p>
<ul>
<li><strong>Stage 1：模型预选</strong> —— 用案例库做相似任务检索，给出top-k候选模型并在<code>train.py</code>中具体化；</li>
<li><strong>Stage 2：代码细化</strong> —— 采用”两阶段轮转搜索（Warm-up&#x2F;Optimization）”，围绕最优候选持续迭代”细化+微调”，若指标提升则保留，否则回退。</li>
</ul>
<p>需要训练学习的是任务模型的参数，即train.py的训练，不需要学习agent模型，agent只做检索、编辑、执行与决策编排。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250903111221289.png" alt="image-20250903111221289"></p>
<p>TS-Agent规划能力的背后，本质上还是要有核心的time series analysis的分析模型、案例和规划数据。如在金融场景下的：<br>①”<strong>Case Bank案例库</strong>“：沉淀过往金融时序任务与成功策略，例如基于案例推理（CBR）做候选模型预选与启发式约束；<br>②”<strong>Financial TS Code Base代码库</strong>“：现成的、能直接用的金融模型代码和评估工具（预测：DLinear&#x2F;TimesNet&#x2F;Autoformer&#x2F;PatchTST&#x2F;TimeMixer；生成：GAN&#x2F;VAE&#x2F;扩散等）；<br>③”<strong>Refinement Knowledge Bank优化知识库</strong>“：金融领域专业操作技巧，模型的训练优化策略（比如怎么处理数据、怎么调参数能减少误差）。</p>
<p>在加密货币、外汇与美股等任务上，TS-Agent相较AutoML与通用Agent基线取得更优的预测精度、风控一致性与成功率。</p>
<h6 id="FLAIRR-TS"><a href="#FLAIRR-TS" class="headerlink" title="FLAIRR-TS"></a>FLAIRR-TS</h6><blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.19279">FLAIRR-TS – Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series</a> 2025.8</p>
</blockquote>
<p><strong>目标</strong>：在无需微调的前提下，通过<strong>检索的可解释外部证据</strong>与<strong>在线提示改写</strong>协同，动态适配不同数据与领域的模式，而非为每个任务手工调参。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250904010220801.png" alt="image-20250904010220801"></p>
<p>FLAIRR-TS将时间序列预测（TSF）表述为一个<strong>多智能体、测试时优化</strong>的问题：由Forecaster生成初始预测；Retriever从历史库检索与当前片段最相似的”类比样本”补充上下文；Refiner作为”元优化器”根据<strong>过往提示词→误差</strong>轨迹，迭代改写提示词并触发早停；整个循环在<strong>不更新权重</strong>的前提下完成（仅通过提示词优化）。</p>
<ol>
<li><strong>Forecaster</strong>生成初始预测，由”当前提示词+检索增强上下文（C_aug）”驱动生成预测。预测目标变量在未来 $H$ 步的数值序列；输出格式被<strong>强制规范</strong>为：<code>Predicted Values: [v1, v2, …]</code>（数值数组），并附带<code>Reasoning</code>、<code>Certainty Estimate</code>与<code>Certainty Reasoning</code>三个解释与置信度字段。</li>
<li><strong>Retriever</strong>依据<strong>皮尔逊相关系数</strong>从训练划分按滑窗构建的历史库中取Top-M与当前上下文最相似的<strong>历史片段</strong>，并连同它们的<strong>真实后续走势</strong>一起提供，作为”<strong>类比演化示例</strong>“，用以<strong>直接增强</strong>Forecaster的输入上下文 $C_{aug}$。</li>
<li><strong>Refiner</strong>作为”元优化器”根据<strong>过往提示词→误差</strong>轨迹，迭代改写提示词并触发早停；整个循环在<strong>不更新权重</strong>的前提下完成（仅通过提示词优化）。</li>
</ol>
<hr>
<h2 id="过往科研工作总结"><a href="#过往科研工作总结" class="headerlink" title="过往科研工作总结"></a>过往科研工作总结</h2><h4 id="Reservoir-Computing-基础"><a href="#Reservoir-Computing-基础" class="headerlink" title="Reservoir Computing 基础"></a>Reservoir Computing 基础</h4><h5 id="模型空间学习-Model-Space-Learning"><a href="#模型空间学习-Model-Space-Learning" class="headerlink" title="模型空间学习 (Model Space Learning)"></a>模型空间学习 (Model Space Learning)</h5><p><strong>1. 为什么要”从数据空间转到模型空间”</strong></p>
<p>真实时序常见问题：<strong>噪声大、采样不齐、序列长短不一、训练样本少、环境在变</strong>。直接在”数据空间”做端到端学习，往往既要解决长依赖与缺失，又要抵抗噪声与域移，训练代价高、可解释性弱。连续时间工作在摘要里把核心痛点概括得很准：不规则采样、样本稀缺、环境变化下的故障诊断；常见插值&#x2F;重采样会<strong>扭曲原始变化信息</strong>，而连续时间网络直接训练又计算开销大。</p>
<p><strong>MSL的动机</strong>就是绕开这些”数据层面的坑”，把”序列→能生成它的一个小模型”，再在”<strong>模型的参数&#x2F;函数</strong>“上做判别或检索。</p>
<p><strong>2. MSL的定义与总体目标</strong></p>
<p><strong>定义</strong>：先为每条序列拟合一个能刻画其<strong>动态生成机制</strong>的”小模型”（如ESN&#x2F;ODE-ESN&#x2F;AR&#x2F;状态空间等），用**”拟合好的模型本身”**当作该序列的表示。</p>
<p><strong>目标</strong>：得到<strong>抗噪声、对采样机制&#x2F;长度不敏感、可解释、少样本也稳定</strong>的时序表征，并显著降低训练代价（比如只训练读出层或在模型参数上做轻量分类）。</p>
<p><strong>3. MSL的流程</strong></p>
<ul>
<li><strong>模型化表征阶段（拟合模型阶段）</strong>：对每条序列，选择一个小而”对口”的生成模型并<strong>单独拟合</strong>。</li>
<li><strong>构建模型度量</strong>：直接用参数向量（如拼接后的Wout）做欧氏&#x2F;马氏距离；或把模型看成函数 $F(\cdot,\Theta)$，以 $|F(\cdot,\Theta_1)-F(\cdot,\Theta_2)|$ 这类<strong>函数距离</strong>比较。</li>
<li><strong>判别式学习阶段</strong>：在”模型特征&#x2F;模型度量”上训练一个轻量分类器或度量学习器即可（弱化了端到端大网络的训练压力）。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250904154520611.png" alt="image-20250904154520611"></p>
<h5 id="ESN高效模型拟合"><a href="#ESN高效模型拟合" class="headerlink" title="ESN高效模型拟合"></a>ESN高效模型拟合</h5><p>回声状态网络（Echo State Network, ESN）是一类简化的离散时间递归神经网络（RNN），主要用于处理具有上下文依赖的序列数据。与传统RNN不同，ESN中输入权重 $W^{hx}$ 和储备池（reservoir）权重 $W^{hh}$ 在初始化时随机生成并在训练过程中保持固定，仅输出权重 $W^{yh}$ 通过训练获得。</p>
<h6 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h6><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250902161843671.png" alt="image-20250902161843671" style="zoom: 25%;" />

<p><strong>输入层</strong>：包含 $K$ 个单元，输入向量表示为<br>$$<br>x(n) &#x3D; \big(x_1(n), x_2(n), \ldots, x_K(n)\big)^{\top} ,<br>$$<br><strong>隐藏层（储备池）</strong>：包含 $N$ 个非线性神经元，其状态为<br>$$<br>h(n) &#x3D; \big(h_1(n), h_2(n), \ldots, h_N(n)\big)^{\top} ,<br>$$<br>在迭代过程中由当前输入和前一时刻的状态共同决定：<br>$$<br>h(n) &#x3D; g!\big(W^{hh}h(n-1) + W^{hx}x(n)\big) ,<br>$$<br>其中 $g(\cdot)$ 通常为 $\tanh$。</p>
<blockquote>
<p>为了保证网络稳定性，储备池必须满足<strong>回声状态性质（Echo State Property, ESP）</strong>，即历史信息对当前状态的影响会随时间衰减。实践中常通过调整谱半径来实现：</p>
<blockquote>
<p>$$<br>W^{hh} \leftarrow \frac{\alpha W^{hh}}{\lVert \lambda_{\max}\rVert}, \quad 0&lt;\alpha&lt;1 ,<br>$$</p>
<p>其中 $\lVert \lambda_{\max}\rVert$ 是 $W^{hh}$ 的谱半径。	</p>
</blockquote>
</blockquote>
<p><strong>输出层</strong>：包含 $L$ 个单元，输出表示为<br>$$<br>y(n) &#x3D; \big(y_1(n), y_2(n), \ldots, y_L(n)\big)^{\top} ,<br>$$</p>
<h6 id="拟合方法-岭回归"><a href="#拟合方法-岭回归" class="headerlink" title="拟合方法-岭回归"></a>拟合方法-岭回归</h6><p>在训练过程中，首先将输入序列逐步送入储备池，得到对应的隐藏状态集合 $H$。然后利用岭回归计算输出权重：<br>$$<br>W^{yh} &#x3D; \big(H^{\top}H + \lambda^2 I\big)^{-1}H^{\top}Y ,<br>$$<br>其中 $Y$ 是目标输出，$\lambda &gt; 0$ 为正则化参数。</p>
<p>综上，ESN能够高效地将带有上下文的时序数据映射到模型空间，并在一定程度上避免了传统RNN的梯度消失问题。</p>
<h4 id="科研工作总结"><a href="#科研工作总结" class="headerlink" title="科研工作总结"></a>科研工作总结</h4><p><strong>1. 融合视觉与动态特征的时序数据异常检测</strong></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10767196">SpaDyn</a></p>
</blockquote>
<ul>
<li><strong>问题</strong>：2D&#x2F;3D GPR中<strong>空间结构+回波动态</strong>信息耦合不足，易漏检&#x2F;误检。</li>
<li><strong>方法</strong>：采用视觉目标检测模型，通过提取视觉特征，进行<strong>异常区域空间定位</strong>。结合水平方向和垂直方向局部动态特征拟合<strong>多方向储备池网络</strong>，对拟合模型进行分类。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250904155717777.png" alt="image-20250904155717777"></p>
<p><strong>2. 基于频谱感知储备池计算网络的时序数据分类</strong></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=DmPW0pO3F3">Spectral-Aware Reservoir Computing（SARC）</a></p>
</blockquote>
<ul>
<li><strong>问题</strong>：传统RC对<strong>多尺度频谱</strong>与<strong>周期&#x2F;序列双通道动力学</strong>利用不足。</li>
<li><strong>方法</strong>：先HP滤波去趋势→小波→FFT选”显著频率”→以频率为周期构造<strong>循环滞后状态</strong>（顺序Ws+周期Wc）、读出层岭回归，<strong>拼接多频动态特征</strong>做TSC；BiESN实现最优（算法&#x2F;结论在文中）。</li>
<li><strong>实验</strong>：在128个UCR&#x2F;UEA数据集上实现<strong>快而准</strong>的分类。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250904155829410.png" alt="image-20250904155829410"></p>
<p><strong>3. 基于连续时间伴随学习的不规则序列故障诊断</strong></p>
<ul>
<li><strong>问题</strong>：<strong>不规则采样&#x2F;连续动力学</strong>下，离散RNN&#x2F;ESN训练与梯度传播不稳定。</li>
<li><strong>方法</strong>：把ESN推广为<strong>连续时间动态方程</strong>，用<strong>伴随法</strong>做稳定高效的神经微分方程优化。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Weichy9218/Image@main/typora/image-20250904155950150.png" alt="image-20250904155950150"></p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">weichy</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://Weichy9218.github.io/2025/09/04/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E8%B0%83%E7%A0%94/">https://Weichy9218.github.io/2025/09/04/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E8%B0%83%E7%A0%94/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">weichy</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/Time-Series-LLM/">
                                    <span class="chip bg-color">Time Series, LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/2025/09/04/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E8%B0%83%E7%A0%94/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/1.jpg" class="responsive-img" alt="Time Series 调研">
                        
                        <span class="card-title">Time Series 调研</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            weichy
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Time-Series-LLM/">
                        <span class="chip bg-color">Time Series, LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2025/07/04/Diffusers/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/5.jpg" class="responsive-img" alt="Diffusers">
                        
                        <span class="card-title">Diffusers</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            weichy
                            
                        </span>
                    </div>
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2025</span>
            
            <span id="year">2025</span>
            <a href="/about" target="_blank">weichy</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2025";
                    var startMonth = "8";
                    var startDate = "17";
                    var startHour = "0";
                    var startMinute = "0";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Weichy9218" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:weichy2023@mail.ustc.edu.cn" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1759520348" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1759520348" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

	
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
